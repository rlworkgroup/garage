@inproceedings{ho2016model,
  title={Model-free imitation learning with policy optimization},
  author={Ho, Jonathan and Gupta, Jayesh and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  pages={2760--2769},
  year={2016},
  url={https://arxiv.org/abs/1605.08478},
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{rubinstein2004cross,
  title={The cross-entropy method: A unified approach to Monte Carlo simulation, randomized optimization and machine learning},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  journal={Information Science \& Statistics, Springer Verlag, NY},
  year={2004}
}

@article{duan2016rl,
  title={RL $\^{} 2$: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@article{haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{rakelly2019efficient,
  title={Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
  author={Kate Rakelly and Aurick Zhou and Deirdre Quillen and Chelsea Finn and Sergey Levine},
  year={2019},
  journal={arXiv preprint arXiv:1903.08254},
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@article{schulman2015trust,
    title={Trust region policy optimization},
    author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
    year={2015},
    eprint={1502.05477},
    journal={arXiv},
}

@article{Fujimoto2018AddressingFA,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Scott Fujimoto and Herke van Hoof and David Meger},
  journal={ArXiv},
  year={2018},
  url={https://arxiv.org/abs/1802.09477},
}

@article{yu2019metaworld,
    title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
    author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
    year={2019},
    journal={arXiv:1910.10897},
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{peters2007reward,
  author={J. {Peters} and S. {Schaal}},
  booktitle={2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
  title={Using Reward-weighted Regression for Reinforcement Learning of Task Space Control},
  year={2007},
  volume={},
  number={},
  pages={262-267},}

@article{2009koberpolicy,
  title = {Policy Search for Motor Primitives in Robotics},
  author = {Kober, J. and Peters, J.},
  journal = {Advances in neural information processing systems 21 : 22nd Annual Conference on Neural Information Processing Systems 2008},
  booktitle = {Advances in neural information processing systems 21},
  pages = {849-856},
  editors = {Koller, D. , D. Schuurmans, Y. Bengio, L. Bottou},
  publisher = {Curran},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  address = {Red Hook, NY, USA},
  month = jun,
  year = {2009},
  month_numeric = {6}
}

@misc{duan2016benchmarking,
      title={Benchmarking Deep Reinforcement Learning for Continuous Control}, 
      author={Yan Duan and Xi Chen and Rein Houthooft and John Schulman and Pieter Abbeel},
      year={2016},
      eprint={1604.06778},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.5555/2980539.2980738,
author = {Kakade, Sham},
title = {A Natural Policy Gradient},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {1531â€“1538},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}
